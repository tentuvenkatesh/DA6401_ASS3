# Venkatesh Tentu-Ma23m025
------------------------------------------------------------------------------------------------------------------------------------
# Transliteration System using RNN, GRU & LSTM with Encoder-Decoder Seq2Seq Models (with and without Attention)

# Wandb Link:
------------------------------------------------------------------------------------------------------------------------------------------
# Dataset
This folder contains data which we used in this Project

# Codes
This folder contains codes(da6401_ass3_a is without attention and da6401_ass3_b is with attention)

# Test data Predicted Datasets
This folder has two csv files,predictions_vanilla.csv(i.e Predictions of test dataset without attention) and predictions_attentions.csv(Predictions of test dataset with attention)
----------------------------------------------------------------------------------------------------------------------------------------------------------
# Project Description

This project aims to build a transliteration system that converts text from one script to another while preserving its phonetic structure. The system leverages advanced neural network architectures, including Recurrent Neural Networks (RNNs), Gated Recurrent Units (GRUs), and Long Short-Term Memory networks (LSTMs). Using encoder-decoder sequence-to-sequence (Seq2Seq) models, the project explores both the conventional approach and enhanced versions incorporating attention mechanisms to improve accuracy.Hyperparameter tuning is done using wandb to find the best performing configurations.
